### 1. Aktualisierte Funktion: run_lasso_for_metrics
# Diese Funktion wurde um den 'fast' Parameter und die 1SE-Stabilitätslogik erweitert.

def run_lasso_for_metrics(df, clinical_df, metrics, pipeline, fast=True):
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import roc_auc_score
    from sklearn.linear_model import LogisticRegressionCV
    from sklearn.base import clone
    import numpy as np

    # Pivot der Daten (Metriken x Bins)
    pivot_df = df.pivot(index="sample", columns="bin_id", values=list(metrics))
    pivot_df.columns = [f"{metric}_{bin_id}" for metric, bin_id in pivot_df.columns]

    # Labels und Stratifizierung vorbereiten
    y = []
    strata = []
    for sample_id in pivot_df.index:
        row = clinical_df[clinical_df["Extracted_ID"] == sample_id].iloc[0]
        is_healthy = row["Patient Type"].lower() == "healthy"
        target_val = 0 if is_healthy else 1
        y.append(target_val)
        if stratify == "Gender":
            strata.append(row["Gender"])
        else:
            strata.append(target_val)

    y = np.array(y)
    X = pivot_df

    # Split in Training und Test (80/20)
    X_train_full, X_test, y_train_full, y_test = train_test_split(
        X, y, test_size=0.2, stratify=strata, random_state=42
    )

    if fast:
        # STAGE 1: Schneller Test auf einem Split zur Vorauswahl
        # Nutzt weniger C-Werte und weniger Folds für Speed
        fast_lasso = LogisticRegressionCV(
            Cs=15, cv=2, penalty='l1', solver='liblinear', scoring='roc_auc', max_iter=2000, random_state=42
        )
        fast_pipeline = clone(pipeline)
        fast_pipeline.steps[-1] = ('lasso_cv', fast_lasso)
        
        fast_pipeline.fit(X_train_full, y_train_full)
        y_prob = fast_pipeline.predict_proba(X_test)[:, 1]
        return {"metrics": metrics, "roc_auc": roc_auc_score(y_test, y_prob)}
    
    # --- STAGE 2: Volle Analyse (für Top 10 Kombinationen) ---
    from cv_lasso_single_fold import cross_validation, analyze_feature_stability
    from sklearn.linear_model import LogisticRegression
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler

    print(f"  > Full benchmarking for {metrics}...", flush=True)
    
    # 1. 5-Fold Cross Validation für Stabilitätsanalyse
    cv_results = cross_validation(X_train_full, y_train_full, pipeline, n_folds=5)
    
    # 2. Stabile Features zählen (Features in allen 5 Folds selektiert)
    stability_df = analyze_feature_stability(cv_results)
    n_stable = len(stability_df[stability_df['Frequency'] == 5]) if not stability_df.empty else 0

    # 3. Bestimmen des 1SE (Parsimonious) C-Werts
    pipeline.fit(X_train_full, y_train_full)
    lasso_cv = pipeline.named_steps['lasso_cv']
    mean_scores = np.mean(lasso_cv.scores_[1], axis=0)
    std_scores = np.std(lasso_cv.scores_[1], axis=0)
    sem_scores = std_scores / np.sqrt(5)
    
    best_idx = np.argmax(mean_scores)
    best_score = mean_scores[best_idx]
    threshold = best_score - sem_scores[best_idx]
    idx_1se = np.where(mean_scores >= threshold)[0][0]
    c_1se = float(lasso_cv.Cs_[idx_1se])

    # 4. "Einfaches" Modell (1SE) fitten zur Berechnung der Ratio
    stable_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('stable_model', LogisticRegression(
            penalty='l1', 
            solver='liblinear', 
            C=c_1se, 
            max_iter=10000, 
            random_state=42
        ))
    ])
    stable_pipeline.fit(X_train_full, y_train_full)

    # 5. Überlapp berechnen (Wie viele Modell-Features sind zugleich unter den Top stabilen?)
    stable_feature_names = set(stability_df[stability_df['Frequency'] == 5]['Feature'])
    
    # Merkmale im sparsamen Modell (1SE)
    pars_feature_names = set(X_train_full.columns[stable_pipeline.named_steps['stable_model'].coef_[0] != 0])
    pars_overlap = pars_feature_names.intersection(stable_feature_names)
    pars_stability_ratio = len(pars_overlap) / len(pars_feature_names) if len(pars_feature_names) > 0 else 0.0

    # Merkmale im besten Modell
    simple_feature_names = set(X_train_full.columns[lasso_cv.coef_[0] != 0])
    simple_overlap = simple_feature_names.intersection(stable_feature_names)
    simple_stability_ratio = len(simple_overlap) / len(simple_feature_names) if len(simple_feature_names) > 0 else 0.0

    y_prob_test = pipeline.predict_proba(X_test)[:, 1]
    test_auc = roc_auc_score(y_test, y_prob_test)

    # 6. C-Variation berechnen (Parameter-Stabilität)
    c_values = [res.get('best_C', np.nan) for res in cv_results]
    c_values = [c for c in c_values if c > 0 and not np.isnan(c)]
    c_variation = np.std(np.log10(c_values)) if len(c_values) > 0 else np.nan

    return {
        "metrics": metrics,
        "n_metrics": len(metrics),
        "n_features": X.shape[1],
        "n_selected_features_best": len(simple_feature_names),
        "simple_features": len(simple_feature_names),
        "simple_stability_ratio": simple_stability_ratio,
        "n_stable": n_stable,
        "pars_features": len(pars_feature_names),
        "pars_stability_ratio": pars_stability_ratio,
        "cv_auc": np.mean([e['auc'] for e in cv_results]),
        "test_auc": test_auc,
        "c_variation": c_variation,
        "best_C": lasso_cv.C_[0]
    }


### 2. Aktualisierter Loop: Zweistufen-Suche
# Erst schnelles Screening (Stage 1), dann Detail-Analyse der Top 10 (Stage 2).

df["bin_id"] = df["chrom"] + "_" + df["start"].astype(str)
metrics_to_test = ["mean", "median", "stdev", "wps_value", "min", "max"]

print("=== STAGE 1: Super Fast Screening (all combinations) ===", flush=True)
results_fast = []
import itertools
for r in range(1, len(metrics_to_test) + 1):
    for combination in itertools.combinations(metrics_to_test, r):
        print(f"Screening combo {combination}...", flush=True)
        res = run_lasso_for_metrics(df, clinical_df, combination, pipeline, fast=True)
        results_fast.append(res)
        print(f"  > Fast AUC: {res['roc_auc']:.3f}", flush=True)

# Auswahl der Top 10 nach AUC aus dem Screening
top_10 = pd.DataFrame(results_fast).sort_values("roc_auc", ascending=False).head(10)
print(f"\nTop 10 candidates found. Starting Stage 2 Deep Analysis...", flush=True)

print("\n=== STAGE 2: Full Benchmarking Top 10 ===", flush=True)
metrics_results = []
for idx, row in top_10.iterrows():
    combo = row['metrics']
    res = run_lasso_for_metrics(df, clinical_df, combo, pipeline, fast=False)
    metrics_results.append(res)

# Ergebnisse speichern und anzeigen
metrics_results = pd.DataFrame(metrics_results).sort_values("cv_auc", ascending=False)
metrics_results.to_csv(f"/labmed/workspace/lotta/finaletoolkit/dataframes_for_ba/lasso_metrics_results_{bin_size}.csv", index=False)

print("\n--- FINAL RESULTS (Top 10) ---", flush=True)
display(metrics_results)

best_metrics = metrics_results.iloc[0]['metrics']
print("\n>>> SELECTED BEST METRICS:", best_metrics, flush=True)
