{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e253b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import importlib\n",
    "from sklearn.base import clone\n",
    "import config\n",
    "importlib.reload(config)\n",
    "from config import BIN_SIZE, ANALYSIS_MODE, SPECIFIC_GROUP, STRATIFY_BY\n",
    "import helper_functions\n",
    "importlib.reload(helper_functions)\n",
    "from helper_functions import preprocess_data, get_fast_pipeline, get_stable_pipeline, get_simple_pipeline\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff54f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Alle FutureWarnings ignorieren\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Optional: Auch ConvergenceWarnings ignorieren (falls LASSO nicht konvergiert)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698de0d2",
   "metadata": {},
   "source": [
    "# 0. Check Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f24edebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_vs_healthy\n",
      "Gastric cancer\n",
      "Gender+Age\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(ANALYSIS_MODE)\n",
    "print(SPECIFIC_GROUP)\n",
    "print(STRATIFY_BY)\n",
    "print(BIN_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b3a227",
   "metadata": {},
   "source": [
    "# 1. Loading of Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1562648",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_path = f\"/labmed/workspace/lotta/finaletoolkit/dataframes_for_ba/final_feature_matrix_gc_corrected_{BIN_SIZE}.tsv\"\n",
    "df = pd.read_csv(matrix_path, sep=\"\\t\")\n",
    "\n",
    "clinical_path = \"/labmed/workspace/lotta/finaletoolkit/dataframes_for_ba/filtered_clinical_characteristics.csv\"\n",
    "clinical_df_raw = pd.read_csv(clinical_path, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4164148f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f523bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 19)\n",
      "(108, 19)\n",
      "99\n",
      "99\n",
      "Number of Samples in Matrix: 198\n",
      "Number of Bins per Sample: 4260.0\n"
     ]
    }
   ],
   "source": [
    "if ANALYSIS_MODE == \"specific_vs_healthy\":\n",
    "    clinical_df = clinical_df_raw[\n",
    "            (clinical_df_raw[\"Patient Type\"] == SPECIFIC_GROUP) |\n",
    "            (clinical_df_raw[\"Patient Type\"].str.lower() == \"healthy\")\n",
    "        ].copy()\n",
    "else:\n",
    "    clinical_df = clinical_df_raw.copy()\n",
    "\n",
    "if STRATIFY_BY ==\"Gender\":\n",
    "    clinical_df = clinical_df[clinical_df[\"Gender\"].isin([\"M\", \"F\"])]\n",
    "if STRATIFY_BY == \"Gender+Age\":\n",
    "    # nehme nur die sample die sowohl age at diagnosis als auch gender haben\n",
    "    clinical_df = clinical_df[clinical_df[\"Age at Diagnosis\"].notna() & clinical_df[\"Gender\"].notna()]\n",
    "else:\n",
    "    clinical_df = clinical_df\n",
    "    \n",
    "# Balancing: Sample as many Healthy as there are Cancer samples\n",
    "cancer_df = clinical_df[clinical_df[\"Patient Type\"].str.lower() != \"healthy\"]\n",
    "print(cancer_df.shape)\n",
    "healthy_df = clinical_df[clinical_df[\"Patient Type\"].str.lower() == \"healthy\"]\n",
    "print(healthy_df.shape)\n",
    "n_cancer = len(cancer_df)\n",
    "\n",
    "healthy_df = healthy_df.sample(n=n_cancer, random_state=42)\n",
    "clinical_df = pd.concat([cancer_df, healthy_df]).copy()\n",
    "print(len(healthy_df))\n",
    "print(len(cancer_df))\n",
    "\n",
    "valid_samples = clinical_df[\"Extracted_ID\"].unique()\n",
    "df = df[df[\"sample\"].isin(valid_samples)].copy()\n",
    "\n",
    "print(f\"Number of Samples in Matrix: {df['sample'].nunique()}\")\n",
    "print(f\"Number of Bins per Sample: {len(df) / df['sample'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f0835",
   "metadata": {},
   "source": [
    "# 2. Age for Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69db8d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median of age at diagnosis is: 64.0\n"
     ]
    }
   ],
   "source": [
    "# Add new columns for stratification fpr age and gender\n",
    "age_at_diagnosis = clinical_df_raw[\"Age at Diagnosis\"]\n",
    "median_age = clinical_df[\"Age at Diagnosis\"].median()\n",
    "print(f\"The median of age at diagnosis is: {median_age}\")\n",
    "clinical_df[\"AgeGroup\"] = pd.cut(\n",
    "\n",
    "    clinical_df[\"Age at Diagnosis\"],\n",
    "    bins=[0, median_age-1, 120],\n",
    "    labels=[f\"<{median_age}\", f\"{median_age}+\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32316ce",
   "metadata": {},
   "source": [
    "# 3. General Function for LASSO perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f8d29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lasso_for_metrics(df, clinical_df, metrics, fast=True):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(df, clinical_df, STRATIFY_BY, metrics)  \n",
    "\n",
    "    if fast:\n",
    "        # STAGE 1: fast screening\n",
    "        fast_pipeline=get_fast_pipeline()\n",
    "        fast_pipeline.fit(X_train, y_train)\n",
    "        y_prob = fast_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        return {\"metrics\": metrics, \n",
    "                \"roc_auc\": roc_auc_score(y_test, y_prob)\n",
    "                }\n",
    "    \n",
    "    # STAGE 2: full benchmarking for top 10 combinations\n",
    "    print(f\"  > Full benchmarking for {metrics}...\", flush=True)\n",
    "\n",
    "\n",
    "    # Declare new pipeline for this run \n",
    "    simple_pipeline = get_simple_pipeline()\n",
    "    simple_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_prob_test = simple_pipeline.predict_proba(X_test)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, y_prob_test)\n",
    "\n",
    "    best_c, c_1se = calculate_cs(simple_pipeline)\n",
    "\n",
    "    # fit stable model (1SE) to calculate the ratio\n",
    "    stable_pipeline = get_stable_pipeline(c_1se)\n",
    "    stable_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    n_stable, n_simple, n_pars, simple_stability_ratio, pars_stability_ratio, c_variation, cv_auc = calculate_stability(X_train, y_train, simple_pipeline, stable_pipeline)\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"n_features\": X.shape[1],\n",
    "        \"n_simple\": n_simple,\n",
    "        \"n_pars\": n_pars,\n",
    "        \"simple_stability_ratio\": simple_stability_ratio,\n",
    "        \"n_pars_features\": len(pars_feature_names),\n",
    "        \"pars_stability_ratio\": pars_stability_ratio,\n",
    "        \"cv_auc\": cv_auc,\n",
    "        \"test_auc\": test_auc,\n",
    "        \"best_C\": best_c,\n",
    "        'c_variation': c_variation\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ac038",
   "metadata": {},
   "source": [
    "# 4. Feature Selektion for LASSO (combinations of metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pivot_step",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 1: Fast Screening (all combinations)\n",
      "Screening combination ('mean',)...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.822\n",
      "Screening combination ('median',)...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.797\n",
      "Screening combination ('stdev',)...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.815\n",
      "Screening combination ('wps_value',)...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.624\n",
      "Screening combination ('min',)...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.564\n",
      "Screening combination ('max',)...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.737\n",
      "Screening combination ('mean', 'median')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.887\n",
      "Screening combination ('mean', 'stdev')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.867\n",
      "Screening combination ('mean', 'wps_value')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.822\n",
      "Screening combination ('mean', 'min')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.777\n",
      "Screening combination ('mean', 'max')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.857\n",
      "Screening combination ('median', 'stdev')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.862\n",
      "Screening combination ('median', 'wps_value')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.724\n",
      "Screening combination ('median', 'min')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.769\n",
      "Screening combination ('median', 'max')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.832\n",
      "Screening combination ('stdev', 'wps_value')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.847\n",
      "Screening combination ('stdev', 'min')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.779\n",
      "Screening combination ('stdev', 'max')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.842\n",
      "Screening combination ('wps_value', 'min')...\n",
      "No NaNs in dataframe\n",
      "  > Fast AUC: 0.529\n",
      "Screening combination ('wps_value', 'max')...\n",
      "No NaNs in dataframe\n"
     ]
    }
   ],
   "source": [
    "### 2. Aktualisierter Loop: Zweistufen-Suche\n",
    "# Erst schnelles Screening (Stage 1), dann Detail-Analyse der Top 10 (Stage 2).\n",
    "\n",
    "from cv_lasso_single_fold import cross_validation, analyze_feature_stability, cv_fold_run, print_performance_table, plot_roc_curves, plot_auc_boxplot\n",
    "\n",
    "df[\"bin_id\"] = df[\"chrom\"] + \"_\" + df[\"start\"].astype(str)\n",
    "metrics_to_test = [\"mean\", \"median\", \"stdev\", \"wps_value\", \"min\", \"max\"]\n",
    "\n",
    "print(\"STAGE 1: Fast Screening (all combinations)\", flush=True)\n",
    "results_fast = []\n",
    "\n",
    "for r in range(1, len(metrics_to_test) + 1):\n",
    "    for combination in itertools.combinations(metrics_to_test, r):\n",
    "        print(f\"Screening combination {combination}...\", flush=True)\n",
    "        res = run_lasso_for_metrics(df, clinical_df, combination, fast=True)\n",
    "        results_fast.append(res)\n",
    "        print(f\"  > Fast AUC: {res['roc_auc']:.3f}\", flush=True)\n",
    "\n",
    "# Auswahl der Top 7 nach AUC aus dem Screening\n",
    "top_10 = pd.DataFrame(results_fast).sort_values(\"roc_auc\", ascending=False).head(10)\n",
    "print(f\"\\nTop 10 candidates found. Starting Stage 2 Deep Analysis...\", flush=True)\n",
    "\n",
    "print(\"\\nSTAGE 2: Full Benchmarking Top 10\", flush=True)\n",
    "metrics_results = []\n",
    "for idx, row in top_10.iterrows():\n",
    "    combination = row['metrics']\n",
    "    res = run_lasso_for_metrics(df, clinical_df, combination, fast=False)\n",
    "    metrics_results.append(res)\n",
    "\n",
    "# Ergebnisse speichern und anzeigen\n",
    "metrics_results = pd.DataFrame(metrics_results).sort_values(\"cv_auc\", ascending=False)\n",
    "metrics_results.to_csv(f\"/labmed/workspace/lotta/finaletoolkit/dataframes_for_ba/lasso_metrics_results_{SPECIFIC_GROUP}_{BIN_SIZE}.csv\", index=False)\n",
    "\n",
    "print(\"\\n--- FINAL RESULTS (Top 10) ---\", flush=True)\n",
    "display(metrics_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics_results.to_csv(f\"/labmed/workspace/lotta/finaletoolkit/dataframes_for_ba/lasso_metrics_results_PanCancer_{bin_size}.csv\", index=False)\n",
    "#metrics_results = pd.read_csv(\"/labmed/workspace/lotta/finaletoolkit/dataframes_for_ba/lasso_metrics_results_PanCancer_10000.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb7b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = metrics_results[\n",
    "   (metrics_results['pars_stability_ratio'] >= 0.1)&\n",
    "    (metrics_results['simple_stability_ratio'] >= 0.1)&\n",
    "    (metrics_results['cv_auc'] >= 0.7)&\n",
    "    (metrics_results['test_auc'] >= 0.7)&\n",
    "    (metrics_results['c_variation'] <= 1)\n",
    "]\n",
    "print(filtered)\n",
    "\n",
    "# nehme die kombi dessen metric anzahl die wenigstens metriken hat, also bevorzuge eine kombi mit 1 metric vor einer kombi mit 2 metriken etc.\n",
    "filtered = filtered.copy()\n",
    "filtered[\"n_metrics\"] = filtered[\"metrics\"].apply(len)\n",
    "filtered = filtered.sort_values(\n",
    "    by=[\"n_metrics\", \"cv_auc\"],  # optional: AUC als Tie-Breaker\n",
    "    ascending=[True, False]\n",
    ")\n",
    "best_metrics = filtered.iloc[0][\"metrics\"]\n",
    "print(\"Final Metric Combination:\", best_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc637011",
   "metadata": {},
   "source": [
    "# 5. Influence of metric selection on model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_explanation",
   "metadata": {},
   "source": [
    "### 5.1 Lasso Modeling with best C parameter \n",
    "\n",
    "The `LogisticRegressionCV` model automatically tried out different values for the parameter `C`. \n",
    "Here we visualize how the accuracy of the model changes with `C`.\n",
    "\n",
    "- **Small C**: Strong regularization (model is “forced” to find simple solutions). Risk of underfitting.\n",
    "- **Large C**: Weak regularization (model can be more complex). Risk of overfitting.\n",
    "- **Best C**: The value that achieved the best balance and thus the highest score in cross-validation (CV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40ce6b",
   "metadata": {},
   "source": [
    "The Reciever operating characteristic curve plots the true positive (TP) rate versus the false positive (FP) rate at different classification thresholds. \n",
    "\n",
    "The thresholds are different probability cutoffs that separate the two classes in binary classification. It uses probability to tell us how well a model separates the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv_lasso_single_fold import cross_validation, analyze_feature_stability\n",
    "\n",
    "best_metrics = ['mean', 'stdev']\n",
    "print(f\"Re-training model with best metrics: {best_metrics}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_data(df, clinical_df, best_metrics)\n",
    "\n",
    "# Fit\n",
    "simple_pipeline = get_simple_pipeline()\n",
    "simple_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Get C values\n",
    "best_c, c_1se = calculate_cs(simple_pipeline)\n",
    "\n",
    "print(f\"Best C (max mean): {best_c:.6f} with AUC: {best_score:.4f}\")\n",
    "print(f\"c_1se (parsimonious): {c_1se:.6f} (Threshold: {threshold:.4f})\")\n",
    "\n",
    "\n",
    "# --- STABILERES MODELL MIT C_1SE ---\n",
    "stable_pipeline = get_stable_pipeline(c_1se)\n",
    "stable_pipeline.fit(X_train, y_train)\n",
    "\n",
    "n_stable, n_simple, n_pars, simple_stability_ratio, pars_stability_ratio, c_variation, cv_auc = calculate_stability(X_train, y_train, simple_pipeline, stable_pipeline)\n",
    "\n",
    "print(f\"n_stable: {n_stable}\")\n",
    "print(f\"n_simple: {n_simple}\")\n",
    "print(f\"n_pars: {n_pars}\")\n",
    "print(f\"simple_stability_ratio: {simple_stability_ratio}\")\n",
    "print(f\"pars_stability_ratio: {pars_stability_ratio}\")\n",
    "print(f\"c_variation: {c_variation}\")\n",
    "print(f\"cv_auc: {cv_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a67409",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Create Common Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Curve 1: Best C (e.g., in Blue)\n",
    "plt.plot(fpr_best, tpr_best, color='red', lw=2, \n",
    "         label=f'Best C Model (AUC = {auc_best:.3f})')\n",
    "\n",
    "# Kurve 2: 1SE Model (z.B. in Grün oder Orange)\n",
    "plt.plot(fpr_1se, tpr_1se, color='green', lw=2,\n",
    "         label=f'1SE Model (AUC = {auc_1se:.3f})')\n",
    "\n",
    "# Diagonale (Zufallslinie)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle=':')\n",
    "\n",
    "plt.title('Comparison: Best C vs. 1SE Lasso Model (Test Set)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- Lasso Parameter Tuning Plot ---\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.semilogx(cs, mean_scores, marker='o', label='Mean CV Score (ROC AUC)')\n",
    "plt.fill_between(cs, mean_scores - sem_scores, mean_scores + sem_scores, alpha=0.2, color='gray', label='1 SEM (Standard Error)')\n",
    "plt.axvline(best_c, color='r', label=f'Best C = {best_c:.3f}')\n",
    "plt.axvline(c_1se, color='g', label=f'1SE C = {c_1se:.4f}')\n",
    "plt.title(\"Lasso Parameter Tuning with 1SE Rule\")\n",
    "plt.xlabel(\"C (Inverse Regularization Strength)\")\n",
    "plt.ylabel(\"CV Score (ROC AUC)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"/labmed/workspace/lotta/finaletoolkit/outputs/plots/lasso_parameter_tuning{bin_size}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ecae4b",
   "metadata": {},
   "source": [
    "## 5.2 Training vs. Test with best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_best_test = simple_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_prob_best_train = simple_pipeline.predict_proba(X_train)[:, 1]\n",
    "\n",
    "fpr_best_test, tpr_best_test, _ = roc_curve(y_test, y_prob_best_test)\n",
    "fpr_best_train, tpr_best_train, _ = roc_curve(y_train, y_prob_best_train)\n",
    "\n",
    "auc_best_train = roc_auc_score(y_train, y_prob_best_train)\n",
    "auc_best_test = roc_auc_score(y_test, y_prob_best_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_best_train, tpr_best_train, color='blue', lw=2, label=f'Train ROC (AUC = {auc_best_train:.3f})')\n",
    "plt.plot(fpr_best_test, tpr_best_test, color='darkorange', lw=2, label=f'Test ROC (AUC = {auc_best_test:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "plt.title('Training vs. Test ROC Performance with Simple Model')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f32f6",
   "metadata": {},
   "source": [
    "## 5.3 Training vs. Test with 1SE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfde1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_1se_test = stable_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_prob_1se_train = stable_pipeline.predict_proba(X_train)[:, 1]\n",
    "\n",
    "fpr_1se_test, tpr_1se_test, _ = roc_curve(y_test, y_prob_1se_test)\n",
    "fpr_1se_train, tpr_1se_train, _ = roc_curve(y_train, y_prob_1se_train)\n",
    "\n",
    "auc_1se_train = roc_auc_score(y_train, y_prob_1se_train)\n",
    "auc_1se_test = roc_auc_score(y_test, y_prob_1se_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_1se_train, tpr_1se_train, color='blue', lw=2, label=f'Train ROC (AUC = {auc_1se_train:.2f})')\n",
    "plt.plot(fpr_1se_test, tpr_1se_test, color='darkorange', lw=2, label=f'Test ROC (AUC = {auc_1se_test:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "plt.title('Training vs. Test ROC Performance with Parsimonious Model')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e99140f",
   "metadata": {},
   "source": [
    "# 6. Selected Important Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dda1e2b",
   "metadata": {},
   "source": [
    "## 6.1 Pipeline with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv_lasso_single_fold import cross_validation, analyze_feature_stability, plot_roc_curves, plot_auc_boxplot\n",
    "\n",
    "lasso_model = simple_pipeline.named_steps['lasso_cv']\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": lasso_model.coef_[0]\n",
    "})\n",
    "print(coef_df.head())\n",
    "important_features = coef_df[coef_df[\"Coefficient\"] != 0].sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(\"SINGLE MODEL (Best C)\")\n",
    "print(f\"Number of Important Features (Best Model): {len(important_features)}\")\n",
    "print(\"\\nTop Features (Best Model - Positive = Indicative for Cancer):\")\n",
    "important_features.head(20).plot.barh(x=\"Feature\", y=\"Coefficient\", title=\"Top Features (Best Model - Positive = Indicative for Cancer)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab868d8",
   "metadata": {},
   "source": [
    "## 6.2 Stable Pipeline with 1SE model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da832023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stable_model = stable_pipeline.named_steps['stable_model']\n",
    "\n",
    "stable_coef_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": stable_model.coef_[0]\n",
    "})\n",
    "\n",
    "stable_important_features = stable_coef_df[stable_coef_df[\"Coefficient\"] != 0].sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(\"STABLE MODEL (c_1se):\")\n",
    "print(f\"Number of Important Features (Stable Model): {len(stable_important_features)}\")\n",
    "print(f\"\\nTop Features (Stable Model - Positive = Indicative for Cancer):\")\n",
    "stable_important_features.head(20).plot.barh(x=\"Feature\", y=\"Coefficient\", title=\"Top Features (Stable Model - Positive = Indicative for Cancer)\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"COMPARISON:\")\n",
    "print(f\"Best C Model: {len(important_features)} features selected\")\n",
    "print(f\"1SE Model:    {len(stable_important_features)} features selected\")\n",
    "print(f\"Difference:   {len(important_features) - len(stable_important_features)} fewer features in 1SE model\")\n",
    "\n",
    "print(f\"stable_important_features: {stable_important_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878391b",
   "metadata": {},
   "source": [
    "# 7. Feature Stability Analysis (Cross-Validation) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc01efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import cv_lasso_single_fold\n",
    "importlib.reload(cv_lasso_single_fold)\n",
    "from cv_lasso_single_fold import plot_roc_curves\n",
    "print(\"Running 5-Fold Cross-Validation for Feature Stability.\")\n",
    "\n",
    "# hier macht es keinen sinn die stable pipeline zu nutzen, da in jedem fold mit dem gleichen c wert (c_1se) trainiert wird\n",
    "cv_results = cross_validation(X_train, y_train, simple_pipeline, n_folds=5)\n",
    "\n",
    "plot_roc_curves(cv_results)\n",
    "plot_auc_boxplot(cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6b9c0",
   "metadata": {},
   "source": [
    "## 7.2 Table with Statistical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv_lasso_single_fold import print_performance_table\n",
    "stat_table = print_performance_table(cv_results)\n",
    "print(stat_table)\n",
    "\n",
    "'''\n",
    "Accuracy: Anteil korrekt klassifizierter Samples\n",
    "Sensitivity: Wie viele Krebs-Patienten wurden erkannt (wichtig!)\n",
    "Specificity: Wie viele Gesunde wurden korrekt erkannt\n",
    "Precision: Von allen als \"Krebs\" vorhergesagten, wie viele waren wirklich Krebs\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b40e2a",
   "metadata": {},
   "source": [
    "## 7.3 Feature Stability Analyse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a68c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_df = analyze_feature_stability(cv_results)\n",
    "\n",
    "stable_in_all = stability_df[stability_df['Frequency'] == 5]\n",
    "print(\"\\nTop Stable Features (Selected across multiple folds):\")\n",
    "print(stability_df.head(5))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "stability_df['Frequency'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Feature Selection Frequency across 5 Folds')\n",
    "plt.xlabel('Number of Folds')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.savefig(f\"/labmed/workspace/lotta/finaletoolkit/outputs/plots/roc_curve_{bin_size}_fold.png\")\n",
    "plt.show()\n",
    "print(f\"Features in ALL 5 folds: {len(stable_in_all)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2ada6",
   "metadata": {},
   "source": [
    "## 7.4 Feature Overlap Heatmap \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = sorted(list({\n",
    "    feat\n",
    "    for e in cv_results\n",
    "    for feat in e['selected_features'].keys()\n",
    "}))\n",
    "\n",
    "matrix = np.zeros((5, len(all_features)))\n",
    "\n",
    "for i, e in enumerate(cv_results):\n",
    "    for j, feat in enumerate(all_features):\n",
    "        if feat in e['selected_features']:\n",
    "            matrix[i, j] = 1\n",
    "\n",
    "# --- Binary colormap ---\n",
    "cmap = ListedColormap([\"#eeeeee\", \"#003366\"])\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "ax = sns.heatmap(\n",
    "    matrix,\n",
    "    cmap=cmap,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cbar=True,\n",
    "    cbar_kws={\"ticks\": [0, 1]}\n",
    ")\n",
    "\n",
    "# Custom colorbar labels\n",
    "colorbar = ax.collections[0].colorbar\n",
    "colorbar.set_ticklabels([\"Not selected\", \"Selected\"])\n",
    "\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Fold\")\n",
    "plt.title(\"Feature Selection Consistency Across Folds\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf08842",
   "metadata": {},
   "source": [
    "## 7.5 Saving stable features in file for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "def extract_genomic_position(feature):\n",
    "    if 'chr' in feature:\n",
    "        return feature[feature.index('chr'):]\n",
    "    return feature\n",
    "\n",
    "metrics = {\n",
    "    \"mean\": \"stable_features_['mean']_50000_fold.csv\",\n",
    "    \"stdev\": \"stable_features_['stdev']_50000_fold.csv\",\n",
    "    \"wps\": \"stable_features_['wps_value']_50000_fold.csv\",\n",
    "    \"mean_median_stdev\": \"stable_features_['mean', 'median', 'stdev']_50000_fold.csv\"\n",
    "}\n",
    "\n",
    "base_path = \"/labmed/workspace/lotta/finaletoolkit/outputs/statistics/\"\n",
    "\n",
    "feature_sets = {}\n",
    "\n",
    "for metric, file in metrics.items():\n",
    "    df = pd.read_csv(base_path + file)\n",
    "    cleaned = {extract_genomic_position(f) for f in df['Feature']}\n",
    "    feature_sets[metric] = cleaned\n",
    "\n",
    "\n",
    "for (m1, f1), (m2, f2) in combinations(feature_sets.items(), 2):\n",
    "    intersection = f1 & f2\n",
    "    print(\n",
    "        f\"Intersection between {m1} and {m2}: \"\n",
    "        f\"{len(intersection)} stable features\\n{intersection}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09af9c",
   "metadata": {},
   "source": [
    "# 8. Visualize the ROC Calculation (Label, Probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200083f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the probabilities for the test set \n",
    "y_prob_test = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. Create a DataFrame to map predictions to sample IDs\n",
    "test_results = pd.DataFrame({\n",
    "    'Sample_ID': X_test.index,\n",
    "    'True_Label': y_test,\n",
    "    'Probability_Cancer': y_prob_test\n",
    "})\n",
    "\n",
    "# 3. Sort the results by probability    \n",
    "test_results = test_results.sort_values(by='Probability_Cancer', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 4. Print the top 5 predictions\n",
    "print(\"Detailed predicitions for test set:\")\n",
    "print(test_results.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a811ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falsch-Negative (Krebs als gesund vorhergesagt)\n",
    "fn_proben = test_results[(test_results['True_Label'] == 1) & (test_results['Probability_Cancer'] < 0.3)]\n",
    "\n",
    "# Falsch-Positive (Gesund als Krebs vorhergesagt)\n",
    "fp_proben = test_results[(test_results['True_Label'] == 0) & (test_results['Probability_Cancer'] > 0.7)]\n",
    "\n",
    "# Merge outliers with test_results to get the probabilities\n",
    "outliers_meta = pd.concat([fn_proben, fp_proben])\n",
    "if not outliers_meta.empty:\n",
    "    ausreisser_klinik = clinical_df.merge(outliers_meta[['Sample_ID', 'Probability_Cancer', 'True_Label']], \n",
    "                                         left_on='Extracted_ID', right_on='Sample_ID')\n",
    "    print(f\"Found outliers with threshold (FN < 0.3, FP > 0.7): {len(ausreisser_klinik)}\")\n",
    "    print(ausreisser_klinik[['Extracted_ID', 'Patient Type', 'Gender', 'Probability_Cancer']])\n",
    "else:\n",
    "    print(\"No outliers found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
